\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[table,xcdraw]{xcolor}
\usepackage{listings}

\usepackage{url}
\usepackage[colorlinks=true]{hyperref}

\lstdefinestyle{pythonstyle}{
	language=Python,
	basicstyle=\ttfamily\small,
	keywordstyle=\color{blue},
	commentstyle=\color{gray},
	stringstyle=\color{teal},
	showstringspaces=false,
	frame=single
}

\lstset{style=pythonstyle}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
	
	\title{Human Activity Recognition Using Smartphone Sensor Data: Analysis and Classification using Azure ML and Power BI
	}
	
	\author{\IEEEauthorblockN{Séamus Knightly}
		\IEEEauthorblockA{\textit{1MECE1} \\
			\textit{University Of Galway}\\
			Galway, Ireland\\
			s.knightly1@universityofgalway.ie}
		\and
		\IEEEauthorblockN{Seán Kelly}
		\IEEEauthorblockA{\textit{1MECE1} \\
			\textit{University Of Galway}\\
			Galway, Ireland \\
			s.kelly178@universityofgalway.ie}
	}
	
	\maketitle
	
	\begin{abstract}
		blah blah
	\end{abstract}
	
	\begin{IEEEkeywords}
		IoT, Remote Patient Monitoring, Accelerometer, Gyroscope, Classification, Assisted Living
	\end{IEEEkeywords}
	
	\section{Introduction}
	
	Human Activity Recognition (HAR) is an important area in the domain of machine learning and medical computing. Focusing on identification and classification of human physical activities based on sensor data. Data for HAR is collected by a wide range of devices, from stationary room sensing video or radar, to wearable or mobile devices. HAR has applications in fields such as remote patient monitoring and assisted living.  
	
	There are a number of large, labelled datasets available for human activity recognition. For this project, the \textit{Human Activity Recognition Using Smartphones} dataset \cite{reyes2013har} is used. This dataset contains sensor data collected from smartphone accelerometers and gyroscopes, as participants performed activities such walking, sitting, and standing. The observations are represented by multiple time and frequency features extracted from the motion signals. Sensor signals from the device’s accelerometer and gyroscope were recorded and processed into 561 time and frequency features.
	
	This dataset is chosen over other datasets, such as HARTH dataset \cite{logacjov2021harth} \cite{bach2021classifier} for its simplicity, requiring only 1 worn sensor. And for the simpler set of classifications, which correspond better with scenarios that would arise in a care facility or other healthcare setting.
	
	This paper aims to analyse and classify human activities using the UCI HAR dataset by developing and evaluating multiple machine learning models in Azure Machine Learning Studio. Power BI is employed to visualise data characteristics, feature distributions, and model performance metrics. The results provide insights into the use of sensor based data and machine learning in Human Activity Recognition.
	
	\section{Dataset Description}
	
	\subsection{Dataset Source}
	
	The dataset used in this study is the \textit{Human Activity Recognition Using Smartphones} dataset \cite{reyes2013har}, first introduced in 2013. Created as part of a study on human centered computing \cite{anguita2013public}, the dataset consists of sensor data collected from 30 volunteers aged between 19 and 48 years, performing six basic activities of daily living.
	
	Each participant carried a Samsung Galaxy S~II smartphone on the waist. The smartphone's embedded accelerometer and gyroscope were used to capture linear acceleration and angular velocity at a constant sampling rate of 50~Hz. The experiment was carried out in a controlled laboratory environment, with participants following a standardised sequence of activities to ensure consistency across samples. The signals were normalised and bounded within 
	[1, -1], and each feature vector is also standardised. 
	
	The introductory paper \cite{anguita2013public} demonstrated that a multiclass Support Vector Machine (SVM) model could achieve an overall classification accuracy of 96\% on this dataset, comparable to or exceeding the performance of systems using specialised wearable sensors. Their work highlighted the feasibility of using simpler accelerometer and gyroscope sensors, such as smart phones, as unobtrusive, affordable, and reliable sensing tools for HAR.

	\subsection{Feature Overview}
	
	The dataset provides both raw sensor readings and pre-processed feature vectors based on that raw data. Each observation represents a 2.56~second window of sensor data with 50\% overlap between adjacent windows, there are 128 readings per window. From each window, 561 features were extracted from the time and frequency domains.
	
	The features include statistical and signal based measures such as mean, standard deviation, median absolute deviation, signal magnitude area, and correlation between sensor axes. Frequency domain features were obtained using Fast Fourier Transform (FFT) on the windowed signals, used to capture additional important motion characteristics for each activity.

	Additionally, subject identifiers (1–30) are included, allowing for subject specific analysis.
	
	In the dataset, each of the recorded activities is encoded numerically as follows: 
	1–\textit{Walking}, 2–\textit{Walking Upstairs}, 3–\textit{Walking Downstairs}, 
	4–\textit{Sitting}, 5–\textit{Standing}, and 6–\textit{Laying}.


	\subsection{Data Preparation}\label{data_preparation}
	
	Prior to analysis, experimentation and model training, a subset of the original dataset features was selected to simplify the analysis and reduce computational overhead. The full UCI HAR dataset contains 561 features derived from both time and frequency transformations. This feature set provides detailed motion characterisation, but it is computationally expensive to process, particularly on the edge in the Rasbperry Pi. 
	
	To address this, a representative subset of features was selected, focusing on descriptive statistics (mean, maximum, and minimum values) from the accelerometer and gyroscope signals.
	
	The retained features include the following categories:
	\begin{itemize}
		\item \textbf{Time domain features:} body acceleration, body acceleration jerk, gyroscope, and gyroscope jerk (mean, max, and min for each axis)
		\item \textbf{Magnitude features:} signal magnitude for acceleration, jerk, and gyroscope signals
		\item \textbf{Frequency domain features:} selected mean, max, and min values for acceleration, jerk, and gyroscope signals
	\end{itemize}
	
	This reduced dataset comprises 101 variables in total. This reduction improves computational efficiency on the edge where limited processing power and energy consumption are key considerations.
	
	The original study \cite{anguita2013public} employed an SVM classifier, which required extensive pre-processed features to achieve high accuracy.  This work explores modern machine learning techniques capable of capturing non-linear relationships and feature interactions. It is therefore expected that the reliance on an extensive set of features is reduced without sacrificing accuracy.
	
	On the Azure Machine Learning side, the \texttt{subject} feature was excluded from model training to generalise the data.

	
	\section{Data Analysis}
	
	\subsection{Data Characteristics}
	
	The class distribution, illustrated in Fig.~\ref{fig:activity-count}, shows that the dataset is well balanced across the six activity categories. Each activity contains roughly a similar number of samples, ensuring that the classification problem is not biased toward any single class. 
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/activity_count_bar_chart.png}
		\caption{Distribution of recorded samples across the six activity classes.}
		\label{fig:activity-count}
	\end{figure}

	
	Figure \ref{fig:gyro-acc-scatter} plots the mean gyroscope signal along the X-axis against the corresponding mean accelerometer signal. The data forms a horizontally elongated cluster centered near 0.25 on the gyroscope axis and 0 on the accelerometer axis. This indicates that angular velocity varies more widely across activities than linear acceleration. The limited vertical spread reflects the fact that participants do not remain perfectly upright.
	The positive offset of the gyroscope mean likely arises because most activities involve forward body motion.
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/scatter_plot_gyro_v_mean.png}
		\caption{Scatter plot showing the relationship between the mean body gyroscope signal (\texttt{tBodyGyro-mean()-X}) and the mean body acceleration signal (\texttt{tBodyAcc-mean()-X}).}
		\label{fig:gyro-acc-scatter}
	\end{figure}
	
	Figure~\ref{fig:feature-histograms} shows the distributions of several accelerometer and gyroscope mean features. 
	The features are bounded within $[-1, 1]$, confirming that normalisation was applied to the dataset to remove sensor bias and ensure comparable scaling. 
	The histograms verify that the dataset is in a state suitable for machine learning analysis.
	
	The frequency domain gyroscope features (\texttt{fBodyGyro-mean()-X} and \texttt{fBodyGyro-mean()-Z}) in the top left of Fig.~\ref{fig:feature-histograms} display an unusual wedge shaped distribution, in contrast to the more symmetric time domain features. This reflects the characteristics of FFT outputs, low frequency components appear more consistently, and high frequency components occur less frequently but with greater variability. 

	The data shows that there is a complementary relationship between time and frequency domain, with time features capturing instantaneous motion and orientation, frequency features show periodicity and energy characteristics. Including both will give the model better information about the motion patterns.
	
	\begin{figure}[t]
		\centering
		\includegraphics[trim=10 95 10 50,clip,width=\linewidth]{figures/histograms.pdf}
		\caption{Histograms of representative accelerometer and gyroscope mean features.}
		\label{fig:feature-histograms}
	\end{figure}

	\subsection{Trends and Patterns}
	
	Figure~\ref{fig:feature-heatmap} illustrates the mean body acceleration and gyroscope signals across all activities.
	The mean acceleration values are relatively uniform across classes.
	Subtle variations are visible in the gyroscope means, particularly along the Z-axis, corresponding to torso rotation and orientation differences during activities such as walking or laying.
	These differences are expected given that the device was worn at the waist, where small postural changes and rotational motion are captured more strongly in the gyroscope signals.
	Overall, the heatmap confirms that the dataset’s body motion features capture the small variations between activity types.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/heatmap.png}
		\caption{Heatmap of selected mean accelerometer and gyroscope features across activity classes.}
		\label{fig:feature-heatmap}
	\end{figure}

	
	\subsection{Feature Comparison}
	
	Figure~\ref{fig:feature-grouped-bar} compares the mean magnitudes of selected motion features across all activity classes. 
	These features, including the body acceleration magnitude, gyroscope magnitude, and jerk magnitudes, represent the overall intensity of body motion.
	
	A separation is visible between dynamic and static activities. 
	\textit{Walking}, \textit{Walking Upstairs}, and \textit{Walking Downstairs} (classes 1–3) exhibit notably higher magnitude values across all features, indicating stronger overall motion and rotational activity. 
	Among these, \textit{Walking Downstairs} shows significantly higher jerk magnitudes, due to the more abrupt deceleration and impact forces during descent. 
	
	In contrast, static postures such as \textit{Sitting}, \textit{Standing}, and \textit{Laying} (classes 4–6) display consistently lower mean magnitudes across all sensors. 
	The near zero variation among these static classes suggests limited body movement and stable device orientation. 
	These results confirm that magnitude based features can capture the motion intensity of each activity and give discriminative information for classification.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/feature_grouped_bar.png}
		\caption{Comparison of mean motion magnitude features across activities.}
		\label{fig:feature-grouped-bar}
	\end{figure}

	
	\subsection{Box Plot Analysis}
	
	Box plots were generated to compare the distributions of selected features across activity categories (Fig.~\ref{fig:boxplot-accmag}). 
	The mean body acceleration magnitude (\texttt{tBodyAccMag-mean()}) has a clear separation between static and dynamic activities. 
	Static postures, \textit{Standing}, \textit{Sitting}, and \textit{Laying}, cluster tightly around $-1$, indicating near zero net body acceleration after gravity removal. 
	In contrast, dynamic activities (\textit{Walking}, \textit{Walking Upstairs}, and \textit{Walking Downstairs}) show higher medians and broader interquartile ranges, as they have higher motion intensity and variation.
	
	For the dynamic categories, \textit{Walking Downstairs} shows the highest median acceleration magnitude, while \textit{Walking Upstairs} has the greatest spread, consistent with the more irregular motion of climbing. 
	Although this feature alone distinguishes static from dynamic activities, the overlap between static classes (e.g., \textit{Sitting} and \textit{Standing}) suggests that multiple features are required for finer classification.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/boxplot_tBodyAccMag.png}
		\caption{Box plot of mean body acceleration magnitude (\texttt{tBodyAccMag-mean()}) across activity classes. The plot was generated using Matplotlib, as Power BI Online does not currently support box plot creation.}
		\label{fig:boxplot-accmag}
	\end{figure}

	
	\subsection{Findings and Discussion}
	
	The data analysis highlights several meaningful trends across the dataset. 
	First, the activity classes are well balanced, ensuring that subsequent classification experiments are not biased toward any single class. 
	Normalisation has been applied to remove sensor bias and facilitate comparison across variables. 
	
	Feature comparisons show separations between static and dynamic activities across both time and frequency domains. 
	Accelerometer features mostly capture translational body motion, making them effective for distinguishing movement, while gyroscope features show the rotation, which separates similar patterns such as \textit{Walking Upstairs} and \textit{Walking Downstairs}. 
	Frequency features provide extra information about periodicity and energy of movement, which will be useful for classification.
	
	As a whole, the data analysis confirms that the selected features make sense, are physically interpretable, and have distinct patterns across activity classes. 
	These characteristics make the dataset suitable for supervised machine learning models in Azure ML studio.

	
	
	\section{Machine Learning Model Development and Evaluation}
	
	Machine learning models were developed and evaluated in Azure ML Studio using the processed dataset described in Section \ref{data_preparation}. A multi-class classification problem was defined, where the target label corresponds to one of six human activity classes. All models were trained using an 70/30 train/test split, and performance was assessed through the use of the \texttt{Evaluate Model} component in ML Studio.
	
	The experiments were conducted according to the project specification.
	
	\subsection{Single Feature Models}
	
	In the first experiment, 99 models were trained using a single feature each from the dataset. This was done using the \textit{Execute Python Script} component in Azure ML Studio, which was useful to automate the mass training and testing of the models. Logistic Regression was used for all, the code for this can be found in Appendix~\ref{single_feature}
	
	The resulting accuracies ranged from $\approx$17\%, roughly equivalent to a random choice, to $\approx$ 52\%, depending on the selected feature. The most informative features were primarily derived from mean accelerometer and gyroscope signals in the time domain, specifically:
	\texttt{fBodyAcc-max()-X}, \texttt{tGravityAcc-min()-Y}, \texttt{tGravityAcc-mean()-Y},
	\texttt{tGravityAcc-max()-Y}, \texttt{tBodyAcc-max()-X}, and \texttt{fBodyGyro-max()-X}.
	These activities had accuracies around $\approx$ 50\%, With Gravity related features being over-represented. This matches the physical expectation however, as these features will distinguish the static and dynamic categories. This instantly gives a boost as it reduces the chance to 1 in 3 on a random choice as 3 of the 6 activities are dynamic.
	
	The top performing single feature model achieved an accuracy of approximately 52.18\%, using \texttt{fBodyAcc-max()-X}. This indicates that horizontal body acceleration alone carries substantial information about activity state, but cannot independently distinguish similar postures such as \textit{Sitting} and \textit{Standing}.
	
	The weakest performing feature was \texttt{tBodyAcc-mean()-Y}, which achieved only 17.1\% accuracy, which is roughly equivalent to random guessing on the 6 activities. This feature captures minimal variation between activities because the Y-axis component of linear acceleration is effected by the constant gravitational force of the earth.
	
	\subsection{Combination of Features}
	
	In the second experiment, a small representative subset of features was used to evaluate how combining motion signals from multiple axes influences classification performance. The selected features represent the mean time domain linear acceleration and angular velocity along the three body axes:
	
	\begin{itemize}
		\item \texttt{tBodyAcc-mean()-X}, \texttt{tBodyAcc-mean()-Y}, \texttt{tBodyAcc-mean()-Z}
		\item \texttt{tBodyGyro-mean()-X}, \texttt{tBodyGyro-mean()-Y}, \texttt{tBodyGyro-mean()-Z}
	\end{itemize}
	
	These six features were chosen because they correspond directly to the type of raw sensor readings that would be available on a typical IoT edge device.
	
	A \textit{Multiclass Logistic Regression} classifier was trained using this six-dimensional feature set. The resulting model achieved test accuracy, precision, and recall values of approximately 27\%, a poor improvement above random guessing. This relatively poor performance indicates that while these features capture fundamental movement patterns, they do not provide sufficient discriminatory information on their own to differentiate between activities.
	
    The limited accuracy can be attributed to several factors. Firstly, mean values omit temporal variation and frequency information that are important for distinguishing activities with similar average motion but different periodic structures, like \textit{Walking Upstairs} and \textit{Walking Downstairs}. Secondly, Logistic Regression, being a linear model, is not able to capture the non linear interactions between accelerometer and gyroscope signals that make up complex human movements, however, incorporating additional derived features like signal magnitude, minimum and maximum values, and frequency domain statistics, can bring out and encode some of these non linear relationships, allowing linear models to achieve improved performance.
	
	\subsection{All Features}
	
	In the third experiment, all 99 available features in the dataset were used to train a \textit{Multiclass Logistic Regression} classifier. This configuration represents the upper bound of model performance using the full set of derived accelerometer and gyroscope features, including both time and frequency statistics such as mean, standard deviation, magnitude, and energy. These features better capture the motion characteristics of human activity.
	
	The trained model achieved an overall test accuracy of 92.8\%, with Micro Precision and Micro Recall values of 0.928, and Macro Precision and Macro Recall values of approximately 0.931 and 0.930, respectively. These results indicate good performance across all six activity classes.
	
	Compared to the previous experiments, the improvement in accuracy demonstrates the importance of including the set of pre-processed features. As discussed previously, the additional features expose the non-linear relationships in the motion signals.
	
	This experiment highlights that high performance can be achieved even with a relatively simple classification algorithm when a sufficiently rich feature set is provided. In IoT, this emphasises the benefit of performing lightweight feature extraction either on the gateway or in the cloud, rather than attempting classification directly on limited raw sensor data at the edge.
	
	\subsection{Feature Selection Methods}
	
	To reduce dimensionality and identify the most informative features, three selection techniques were evaluated in Azure ML Studio: \textit{Permutation Feature Importance}, and \textit{Filter Based Feature Selection} using the \textit{Pearson Correlation} and \textit{Chi-Squared} criteria (using a \texttt{Permutation Feature Importance} component and \texttt{Filter Based Feature Selection} components) Each method was followed by model retraining to assess its effect on performance.
	
	\textbf{Permutation Feature Importance (PFI)} was applied using a trained \textit{Multiclass Decision Forest}, achieving an accuracy of 96.1\%. The most influential features included \texttt{tGravityAcc-min()-X}, \texttt{tGravityAcc-min()-Y}, \texttt{tBodyAcc-max()-X}, and \texttt{fBodyAccMag-max()}.
	
	Using \textbf{Filter Based Selection}, the \textit{Pearson Correlation} method selected ten features with the highest linear correlation to activity class, achieving 59.6\% accuracy. In contrast, the \textit{Chi-Squared} method achieved a higher accuracy of 81.7\%, identifying both time and frequency domain features such as \texttt{tBodyAcc-max()-X}, \texttt{tGravityAccMag-max()}, and \texttt{fBodyAccMag-mean()} as most relevant.
	
	A summary of results is given in Table~\ref{tab:feature-selection-results}. The PFI method with a Decision Forest provided the best overall accuracy. Chi-Squared filtering offered a good compromise between simplicity and accuracy, making it suitable for resource limited IoT applications.
	
	\begin{table}[t]
		\caption{Feature selection method performance summary.}
		\label{tab:feature-selection-results}
		\centering
		\begin{tabular}{|l|c|}
			\hline
			\textbf{Method} & \textbf{Accuracy (\%)} \\ \hline
			Permutation Feature Importance (Decision Forest) & 96.1 \\ \hline
			Filter Based (Pearson Correlation) & 59.6 \\ \hline
			Filter Based (Chi-Squared) & 81.7 \\ \hline
		\end{tabular}
	\end{table}

	
	
	\section{Experiment Results Comparison and Discussion}
	
	Table~\ref{tab:comparison-summary} summarises the performance of all experiments conducted in Azure ML Studio. The progression from single features to the full feature set demonstrates a clear relationship between feature richness, model complexity, and classification accuracy.
	
	\begin{table}[t]
		\caption{Summary of model performance across all experiments.}
		\label{tab:comparison-summary}
		\centering
		\begin{tabular}{|l|c|}
			\hline
			\textbf{Model / Feature Set} & \textbf{Accuracy (\%)} \\ \hline
			Single Feature (best: \texttt{fBodyAcc-max()-X}) & 52.2 \\ \hline
			Six Mean Features (Accelerometer + Gyroscope) & 27.0 \\ \hline
			All 99 Features (Logistic Regression) & 92.8 \\ \hline
			Filter Based (Pearson Correlation) & 59.6 \\ \hline
			Filter Based (Chi-Squared) & 81.7 \\ \hline
			Permutation Feature Importance (Decision Forest) & 96.1 \\ \hline
		\end{tabular}
	\end{table}
	
	The single-feature and small-subset experiments highlight that limited raw sensor data provide only partial information about human activity. Individual features could distinguish static from dynamic motion but failed to capture finer variations between activities such as \textit{Walking Upstairs} and \textit{Walking Downstairs}. The poor accuracy of the six-feature model (27\%) reinforces this limitation and reflects the reduced representational power of mean values that discard temporal and frequency variation.
	
	In contrast, the full feature model achieved 92.8\% accuracy, showing that the engineered features available in the dataset significantly improve separability. Derived statistical, magnitude, and frequency domain attributes expose latent non-linear relationships that even a linear classifier like Logistic Regression can exploit. This demonstrates that well-designed feature extraction can compensate for model simplicity.
	
	The feature-selection experiments further confirmed this relationship. The Chi-Squared filter retained the most relevant subset of features, achieving 81.7\% accuracy—close to the full model but with much lower dimensionality. The Pearson correlation filter, limited to linear dependencies, performed poorly at 59.6\%. The Permutation Feature Importance method, evaluated using a Decision Forest, produced the highest overall accuracy (96.1\%), reflecting the capability of ensemble models to capture non-linear feature interactions without explicit feature engineering.
	
	From an IoT system design perspective, these results highlight several key trade-offs:
	\begin{itemize}
		\item \textbf{Edge vs. Cloud Processing:} Lightweight models on raw sensor data achieve limited accuracy and are suitable only for coarse classification. More complex processing, including feature extraction and model inference, should be offloaded to a cloud platform or high-performance gateway to achieve reliable recognition.
		\item \textbf{Feature Dimensionality:} While the full 99-feature model delivers high accuracy, feature selection can substantially reduce computation and bandwidth costs with minimal performance loss—important for constrained IoT deployments.
		\item \textbf{Model Choice:} Linear models such as Logistic Regression are interpretable and efficient but limited in expressiveness. Non-linear models like Decision Forests or Neural Networks provide superior performance at the cost of higher computational load.
	\end{itemize}
	
	Overall, the experiments demonstrate that high classification accuracy in Human Activity Recognition depends primarily on the availability of informative features. Combining moderate feature selection with non-linear classifiers offers an optimal balance between computational efficiency and predictive accuracy, aligning with the design goals of scalable IoT systems.

	\section{Documentation of Web Service Deployment, Use, and Visualisation}
	
	\subsection{Deploying a Machine Learning Model as a Web Service}
	To deploy a machine learning model as a service, the first step is to create a real time inference pipeline from the pipeline that generated the model. Figure \ref{fig:pipeline-to-inference} shows the design of the original pipeline, and the resultant real time inference pipeline. Two components, \texttt{Web Service Input} and \texttt{Web Service Output}, have been added, to allow for querying an API and receiving a response.
	
	\begin{figure}[t]
		\centering
		
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/pipeline.png}
			\textbf{(a) Pipeline}
		\end{minipage}
		\hfill
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/inference.png}
			\textbf{(b) Inference Pipeline}
		\end{minipage}
		
		\caption{Pipeline to inference workflow}
		\label{fig:pipeline-to-inference}
	\end{figure}

	Once the Real Time Inference Pipeline has been created and completes its submission job, it will need to be deployed to a an endpoint, which will expose the model’s prediction logic through a REST API. The endpoint must be created first, and then the inference pipeline must be deployed to it. Once the endpoint is ready, it will be in the final "Healthy" state and inspecting the details page will give a similar output to that shown in Figure~\ref{fig:endpoint} 
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/real_time_inference_endpoint.png}
		\caption{Details Page for Endpoint}
		\label{fig:endpoint}
	\end{figure}
	
	After the endpoint becomes healthy, the REST API can be queried directly. Azure also exposes an auto-generated Swagger specification for the endpoint, which documents the request and response formats that the service expects. This specification is useful for understanding the required JSON structure, supported operations, and example payloads. The start of the Swagger documentation for this deployment is shown in Figure~\ref{fig:swagger}.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/swagger_doc.png}
		\caption{Swagger Output}
		\label{fig:swagger}
	\end{figure}
	
	\subsection{Creating and Running a Stream Analytics Job}
	With the model now exposed through an endpoint, the next stage is to create a Azure Stream Analytics (ASA) job.
	
	After creating the ASA job, it needs to be given an input to take in IoT device data, an output to send the data on to Power BI for visualisation, a function to query the REST API, and an SQL query to transfer the information from input to output while also including the function output. 
	
	First, create an input, ensure that IoT Hub is chosen as the input type, and give it a unique name. The creation window will look like Figure~\ref{fig:creating_input}. Give it a unique name and choose the IoT Hub that the IoT device is sending to.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.2\linewidth]{figures/creating_input.png}
		\caption{Creating an ASA input.}
		\label{fig:creating_input}
	\end{figure}
	
	Next, create a function. When adding the function, choose "Azure ML Service". The creation window will look like Figure~\ref{fig:function_creation}. Give it a unique name, and take note of the function signature, as this will be used to craft the SQL query.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.2\linewidth]{figures/function_creation.png}
		\caption{Creating an ASA function.}
		\label{fig:function_creation}
	\end{figure}
	
	Before completing the next task, create a workspace in Power BI, or take note of a workspace that is in use. Take note of the workspace name. Figure~\ref{fig:power_bi_workspace_creation} shows what the creation window looks like in Power BI.
	
	Creating the workspace is necessary because Azure Stream Analytics writes its output directly into a Power BI dataset. The workspace selected determines where the real time dataset will appear and where the final visualisations will be built.
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.2\linewidth]{figures/creating_workspace_power_bi.png}
		\caption{Power BI Workspace Creation.}
		\label{fig:power_bi_workspace_creation}
	\end{figure}
	
	Next, back in the Stream Analytics Job, create an output. Choose Power BI when adding the output. Give a unique name, and choose the workspace from the Power BI account. Give a name to the dataset and the table. There will also be a need to Authorise the connection to Power BI. The creation window is shown in Figure~\ref{fig:adding_output}.
	
	The Power BI output defines the destination for the model predictions produced by the Azure ML function. Once the connection is authorised, Stream Analytics will push each prediction into the specified dataset and table in real time, allowing Power BI to automatically update dashboards and reports as new IoT messages arrive.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.3\linewidth]{figures/creating_output.png}
		\caption{Adding a Power BI output.}
		\label{fig:adding_output}
	\end{figure}
	
	Finally, create an SQL query. This is done last as the names of the inputs, outputs, and functions must be known in order to create the query.
	
	The SQL query acts as the transformation and routing layer between the  IoT telemetry, the Azure ML endpoint, and the final output consumed by Power BI. It must reshape the raw device messages into the JSON structure expected by the REST API, call the machine learning function using that payload, and then format the prediction results into a table that can be visualised in Power BI. The full query used in this project is shown in Appendix~\ref{azure_sql}, where each clause ensures the correct mapping of fields from the IoT device to the model input and the extraction of the returned prediction.
	
	To note here, Power BI only supports a maximum of 75 columns when it is receiving the data, so the data sent to Power BI only includes a subset of features, and the prediction label. Also note that it is important to send the timestamps for the data, to allow for visualising in Power BI.
	
	The ASA Job is now ready to be started. After starting the job, the overview page will look like Figure~\ref{fig:asa_job}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/asa_overview_page.png}
		\caption{Stream Analytics Job Overview Page when Job is running.}
		\label{fig:asa_job}
	\end{figure}
	
	\subsection{Visualising in Power BI}
	Once the IoT device has started transmitting, and the previous steps have been completed, the dataset specified will start receiving data in Power BI. 
	
	First, go to the workspace that the data is being sent to, there will now be a new dataset, with the same name as what was specified previously. Figure~\ref{fig:power_bi_dataset} shows what the workspace might look like now in Power BI. 
	
	\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/workspace_with_dataset.png}
	\caption{Power BI Workspace showing the new dataset from the ASA Job.}
	\label{fig:power_bi_dataset}
	\end{figure}	
	
	Next, create a report on the dataset. 
	
	With the editing view, the data pane is the right, the data should be visible in the format specified in the SQL query created as part of the ASA job creation step, this can be seen in Figure~\ref{fig:bare_report} 
	
	\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/report_creation_page.png}
	\caption{Power BI Report Creation Workspace with data on the right.}
	\label{fig:bare_report}
	\end{figure}		
	 
	Now, various visuals for the data are available in the \texttt{Visualizations} tab. For this project, two \texttt{Line Chart}s and a \texttt{Card} are chosen. These can be dragged into the workspace. 
	
	For the \texttt{Line Chart}s, in the case of the data for this project, the \texttt{EventEnqueuedUtcTime} time was chosen for the X-axis for both charts. Then each chart displayed the X, Y and Z axes for Gyroscopic and Accelerometer Data respectively. To make the dashboard display the newest information, a filter was applied to both of these dashboard to only display data from the past minute. This was done by applying a filter, in the \texttt{Filters} tab, on \texttt{EventEnqueuedUtcTime} for both charts.
	
	The \texttt{Card} was then used to display the last \texttt{activity\_label}, therefore displaying the current activity.
	
	The final visualisation can be seen in Figure~\ref{fig:power_bi_visualisation}.
	
	
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figures/power_bi_final_visual.png}
		\caption{Final Power BI Visualisation}
		\label{fig:power_bi_visualisation}
	\end{figure}
	
	\bibliographystyle{IEEEtran}
	\bibliography{references}
	
	\onecolumn
	\appendices
	\section{Script Used for Single Feature Evaluation in \texttt{Exectute Python Script} Component}\label{single_feature}
	\begin{lstlisting}[language=Python]
	import pandas as pd
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	from sklearn.metrics import accuracy_score
	
	# The script MUST contain a function named azureml_main
	# which is the entry point for this module.

	
	# The entry point function MUST have two input arguments.
	# If the input port is not connected, the corresponding
	# dataframe argument will be None.
	#   Param<dataframe1>: a pandas.DataFrame
	#   Param<dataframe2>: a pandas.DataFrame
	def azureml_main(dataframe1 = None, dataframe2 = None):
	
	df = dataframe1.copy() # copy dataset
	y = df['activity'] # activity is the category
	X = df.drop(columns=['activity']) # Dataset
	
	results = []
	
	# iterate over the features available
	for feature in X.columns: 
		X_single = X[[feature]]
		X_train, X_test, y_train, y_test = train_test_split(
			X_single, y, test_size=0.2, random_state=42
		)
		# Train a logistic regression model
		model = LogisticRegression(max_iter=1000)
		model.fit(X_train, y_train)
		# grab the accuracy
		acc = accuracy_score(y_test, model.predict(X_test))
		results.append({'Feature': feature, 'Accuracy': acc})
	
	results_df = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)
	
	# output the results dataframe
	return results_df,
	
	
	\end{lstlisting}
	
	\section{Azure Stream Analytics job Query}\label{azure_sql}
	\begin{lstlisting}[language=SQL]
		WITH FeaturesWithTime AS (
			SELECT
				EventEnqueuedUtcTime,
				activity,
				[tBodyAcc-mean()-X],
				[tBodyAcc-mean()-Y],
				[tBodyAcc-mean()-Z],
				[tBodyAcc-max()-X],
				[tBodyAcc-max()-Y],
				[tBodyAcc-max()-Z],
				[tBodyAcc-min()-X],
				[tBodyAcc-min()-Y],
				[tBodyAcc-min()-Z],
				[tBodyAccJerk-mean()-X],
				[tBodyAccJerk-mean()-Y],
				[tBodyAccJerk-mean()-Z],
				[tBodyAccJerk-max()-X],
				[tBodyAccJerk-max()-Y],
				[tBodyAccJerk-max()-Z],
				[tBodyAccJerk-min()-X],
				[tBodyAccJerk-min()-Y],
				[tBodyAccJerk-min()-Z],
				[tBodyGyro-mean()-X],
				[tBodyGyro-mean()-Y],
				[tBodyGyro-mean()-Z],
				[tBodyGyro-max()-X],
				[tBodyGyro-max()-Y],
				[tBodyGyro-max()-Z],
				[tBodyGyro-min()-X],
				[tBodyGyro-min()-Y],
				[tBodyGyro-min()-Z],
				[tBodyGyroJerk-mean()-X],
				[tBodyGyroJerk-mean()-Y],
				[tBodyGyroJerk-mean()-Z],
				[tBodyGyroJerk-max()-X],
				[tBodyGyroJerk-max()-Y],
				[tBodyGyroJerk-max()-Z],
				[tBodyGyroJerk-min()-X],
				[tBodyGyroJerk-min()-Y],
				[tBodyGyroJerk-min()-Z],
				[tBodyAccMag-mean()],
				[tBodyAccMag-max()],
				[tBodyAccMag-min()],
				[tBodyAccJerkMag-mean()],
				[tBodyAccJerkMag-max()],
				[tBodyAccJerkMag-min()],
				[tBodyGyroMag-mean()],
				[tBodyGyroMag-max()],
				[tBodyGyroMag-min()],
				[tBodyGyroJerkMag-mean()],
				[tBodyGyroJerkMag-max()],
				[tBodyGyroJerkMag-min()],
				[fBodyAcc-mean()-X],
				[fBodyAcc-mean()-Y],
				[fBodyAcc-mean()-Z],
				[fBodyAcc-max()-X],
				[fBodyAcc-max()-Y],
				[fBodyAcc-max()-Z],
				[fBodyAcc-min()-X],
				[fBodyAcc-min()-Y],
				[fBodyAcc-min()-Z],
				[fBodyAccJerk-mean()-X],
				[fBodyAccJerk-mean()-Y],
				[fBodyAccJerk-mean()-Z],
				[fBodyAccJerk-max()-X],
				[fBodyAccJerk-max()-Y],
				[fBodyAccJerk-max()-Z],
				[fBodyAccJerk-min()-X],
				[fBodyAccJerk-min()-Y],
				[fBodyAccJerk-min()-Z],
				[fBodyGyro-mean()-X],
				[fBodyGyro-mean()-Y],
				[fBodyGyro-mean()-Z],
				[fBodyGyro-max()-X],
				[fBodyGyro-max()-Y],
				[fBodyGyro-max()-Z],
				[fBodyGyro-min()-X],
				[fBodyGyro-min()-Y],
				[fBodyGyro-min()-Z],
				[fBodyAccMag-mean()],
				[fBodyAccMag-max()],
				[fBodyAccMag-min()],
				[fBodyBodyAccJerkMag-mean()],
				[fBodyBodyAccJerkMag-max()],
				[fBodyBodyAccJerkMag-min()],
				[fBodyBodyGyroMag-mean()],
				[fBodyBodyGyroMag-max()],
				[fBodyBodyGyroMag-min()],
				[fBodyBodyGyroJerkMag-mean()],
				[fBodyBodyGyroJerkMag-max()],
				[fBodyBodyGyroJerkMag-min()]
			FROM
				[activity-recognition-hub]
		),
		
		Features AS (
			SELECT
				activity,
				[tBodyAcc-mean()-X],
				[tBodyAcc-mean()-Y],
				[tBodyAcc-mean()-Z],
				[tBodyAcc-max()-X],
				[tBodyAcc-max()-Y],
				[tBodyAcc-max()-Z],
				[tBodyAcc-min()-X],
				[tBodyAcc-min()-Y],
				[tBodyAcc-min()-Z],
				[tBodyAccJerk-mean()-X],
				[tBodyAccJerk-mean()-Y],
				[tBodyAccJerk-mean()-Z],
				[tBodyAccJerk-max()-X],
				[tBodyAccJerk-max()-Y],
				[tBodyAccJerk-max()-Z],
				[tBodyAccJerk-min()-X],
				[tBodyAccJerk-min()-Y],
				[tBodyAccJerk-min()-Z],
				[tBodyGyro-mean()-X],
				[tBodyGyro-mean()-Y],
				[tBodyGyro-mean()-Z],
				[tBodyGyro-max()-X],
				[tBodyGyro-max()-Y],
				[tBodyGyro-max()-Z],
				[tBodyGyro-min()-X],
				[tBodyGyro-min()-Y],
				[tBodyGyro-min()-Z],
				[tBodyGyroJerk-mean()-X],
				[tBodyGyroJerk-mean()-Y],
				[tBodyGyroJerk-mean()-Z],
				[tBodyGyroJerk-max()-X],
				[tBodyGyroJerk-max()-Y],
				[tBodyGyroJerk-max()-Z],
				[tBodyGyroJerk-min()-X],
				[tBodyGyroJerk-min()-Y],
				[tBodyGyroJerk-min()-Z],
				[tBodyAccMag-mean()],
				[tBodyAccMag-max()],
				[tBodyAccMag-min()],
				[tBodyAccJerkMag-mean()],
				[tBodyAccJerkMag-max()],
				[tBodyAccJerkMag-min()],
				[tBodyGyroMag-mean()],
				[tBodyGyroMag-max()],
				[tBodyGyroMag-min()],
				[tBodyGyroJerkMag-mean()],
				[tBodyGyroJerkMag-max()],
				[tBodyGyroJerkMag-min()],
				[fBodyAcc-mean()-X],
				[fBodyAcc-mean()-Y],
				[fBodyAcc-mean()-Z],
				[fBodyAcc-max()-X],
				[fBodyAcc-max()-Y],
				[fBodyAcc-max()-Z],
				[fBodyAcc-min()-X],
				[fBodyAcc-min()-Y],
				[fBodyAcc-min()-Z],
				[fBodyAccJerk-mean()-X],
				[fBodyAccJerk-mean()-Y],
				[fBodyAccJerk-mean()-Z],
				[fBodyAccJerk-max()-X],
				[fBodyAccJerk-max()-Y],
				[fBodyAccJerk-max()-Z],
				[fBodyAccJerk-min()-X],
				[fBodyAccJerk-min()-Y],
				[fBodyAccJerk-min()-Z],
				[fBodyGyro-mean()-X],
				[fBodyGyro-mean()-Y],
				[fBodyGyro-mean()-Z],
				[fBodyGyro-max()-X],
				[fBodyGyro-max()-Y],
				[fBodyGyro-max()-Z],
				[fBodyGyro-min()-X],
				[fBodyGyro-min()-Y],
				[fBodyGyro-min()-Z],
				[fBodyAccMag-mean()],
				[fBodyAccMag-max()],
				[fBodyAccMag-min()],
				[fBodyBodyAccJerkMag-mean()],
				[fBodyBodyAccJerkMag-max()],
				[fBodyBodyAccJerkMag-min()],
				[fBodyBodyGyroMag-mean()],
				[fBodyBodyGyroMag-max()],
				[fBodyBodyGyroMag-min()],
				[fBodyBodyGyroJerkMag-mean()],
				[fBodyBodyGyroJerkMag-max()],
				[fBodyBodyGyroJerkMag-min()]
			FROM
				FeaturesWithTime
		)
		
		SELECT
			FWT.EventEnqueuedUtcTime,
			FWT.[tBodyAcc-mean()-X],
			FWT.[tBodyAcc-mean()-Y],
			FWT.[tBodyAcc-mean()-Z],
			FWT.[tBodyAccJerk-mean()-X],
			FWT.[tBodyGyro-mean()-X],
			FWT.[tBodyGyro-mean()-Y],
			FWT.[tBodyGyro-mean()-Z],
			FWT.[tBodyAccMag-mean()],
			FWT.[tBodyAccMag-max()],
			FWT.[tBodyGyroMag-mean()],
			FWT.[fBodyAcc-mean()-X],
			FWT.[fBodyAcc-mean()-Y],
			FWT.[fBodyAcc-mean()-Z],
			FWT.[fBodyAccJerk-mean()-X],
			FWT.[fBodyGyro-mean()-X],
			FWT.[fBodyBodyGyroMag-mean()],
			FWT.[fBodyAccMag-mean()],
			FWT.[fBodyBodyGyroJerkMag-mean()],
			([do-recognise-activity](F)).[Scored Labels] AS result
		INTO [My-Workspace]
			FROM FeaturesWithTime FWT
		JOIN Features F ON DATEDIFF(second, FWT, F) BETWEEN 0 AND 1;
	\end{lstlisting}


\end{document}